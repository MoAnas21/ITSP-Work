{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\moham\\anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import tflearn\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.estimator import regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_detection(img):\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    face_cordinates = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    faces = []\n",
    "    if len(face_cordinates)>0:\n",
    "        no_of_faces = face_cordinates.shape[0]\n",
    "    else:\n",
    "        no_of_faces = 0\n",
    "    for i in range(no_of_faces):\n",
    "        faces.append(img[face_cordinates[i][1]:face_cordinates[i][1]+face_cordinates[i][3],face_cordinates[i][0]:face_cordinates[i][0]+face_cordinates[i][2],:])\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(name):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    i = 1\n",
    "    try:\n",
    "        path = os.curdir + \"/data/\" + name\n",
    "        os.mkdir(path) \n",
    "    except FileExistsError:\n",
    "        print(\"Data with the name \" , name ,  \" already exists\")\n",
    "        return\n",
    "    while(cap.isOpened() and i<100):\n",
    "        ret, frame = cap.read()\n",
    "        if ret == True:\n",
    "            cv2.imshow('Frame', frame)\n",
    "            faces = face_detection(frame)\n",
    "            if len(faces)>0:\n",
    "                cv2.imwrite(path + \"/\" + str(i) + \".jpg\",faces[0])\n",
    "            if cv2.waitKey(50) & 0xFF == ord('q'):\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "        i = i+1\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognise_face():\n",
    "    names = os.listdir(\"Data\")\n",
    "    train_images = np.empty((0,50,50,1), dtype=np.float64)\n",
    "    temp_array = np.empty((50,50,1), dtype=np.float64)\n",
    "    train_labels = np.empty((0,len(names)),dtype=np.uint8)\n",
    "    label_array = np.empty((len(names)), dtype=np.uint8)\n",
    "    no_data = 0\n",
    "    for i in range(len(names)):\n",
    "        images = os.listdir(\"Data/\" + names[i])\n",
    "        label_array = [0 for i in range(len(names))]\n",
    "        label_array[i] = 1 \n",
    "        for j in range(len(images)):\n",
    "            clear_output(True)\n",
    "            print(str(no_data))\n",
    "            img = cv2.imread(\"Data/\" + names[i] + \"/\" + images[j], 1)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            img = cv2.resize(img,(50,50))\n",
    "            temp_array[:,:,0] = img\n",
    "            train_images = np.append(train_images,np.array([temp_array]),axis=0)\n",
    "            train_labels = np.append(train_labels,np.array([label_array]),axis=0)\n",
    "            no_data = no_data + 1\n",
    "    train_images = train_images\n",
    "\n",
    "    ops.reset_default_graph()\n",
    "    convnet = input_data(shape=[50,50,1])\n",
    "    convnet = conv_2d(convnet, 32, 5, activation='relu')\n",
    "    convnet = max_pool_2d(convnet, 5)\n",
    "    convnet = conv_2d(convnet, 64, 5, activation='relu')\n",
    "    convnet = max_pool_2d(convnet, 5)\n",
    "    convnet = conv_2d(convnet, 128, 5, activation='relu')\n",
    "    convnet = max_pool_2d(convnet, 5)\n",
    "    convnet = conv_2d(convnet, 64, 5, activation='relu')\n",
    "    convnet = max_pool_2d(convnet, 5)\n",
    "    convnet = conv_2d(convnet, 32, 5, activation='relu')\n",
    "    convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "    convnet = fully_connected(convnet, 1024, activation='relu')\n",
    "    convnet = dropout(convnet, 0.8)\n",
    "    convnet = fully_connected(convnet, len(names), activation='softmax')\n",
    "    convnet = regression(convnet, optimizer='adam', learning_rate = 0.001, loss='categorical_crossentropy')\n",
    "    model = tflearn.DNN(convnet, tensorboard_verbose=1)\n",
    "    history = model.fit(train_images, train_labels, n_epoch=15, show_metric = True, run_id=\"FRS\" )\n",
    "    model.save(\"model.tfl\")\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    temp_array = np.empty((50,50,1), dtype=np.float64)\n",
    "    while(cap.isOpened()):\n",
    "        ret, frame = cap.read()\n",
    "        if ret == True:\n",
    "            cv2.imshow('Frame', frame)\n",
    "            faces = face_detection(frame)\n",
    "            if len(faces)>0:\n",
    "                img = cv2.cvtColor(faces[0], cv2.COLOR_BGR2GRAY)\n",
    "                img = cv2.resize(img,(50,50))\n",
    "                temp_array[:,:,0] = img\n",
    "                predictions = model.predict([temp_array])\n",
    "                print(names[np.argmax(predictions[0])])\n",
    "            if cv2.waitKey(50) & 0xFF == ord('q'):\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr \n",
    "from difflib import get_close_matches \n",
    "import threading\n",
    "\n",
    "commands = [\"add\",\"find\"]\n",
    "\n",
    "def closeMatches(patterns, word):\n",
    "    data = word.split()\n",
    "    for temp in data: \n",
    "        match_list = get_close_matches(temp, patterns)\n",
    "        if len(match_list) != 0:\n",
    "            return match_list[0]\n",
    "    return 1\n",
    "\n",
    "def detect_audio():\n",
    "    r = sr.Recognizer() \n",
    "    mic = sr.Microphone() \n",
    "    with mic as source: \n",
    "        r.adjust_for_ambient_noise(source) \n",
    "        print(\"Say Something\")\n",
    "        audio = r.listen(source) \n",
    "        try: \n",
    "            text = r.recognize_google(audio)\n",
    "            return text \n",
    "        except sr.UnknownValueError: \n",
    "            return 1 \n",
    "        return -1\n",
    "    \n",
    "def run():\n",
    "    instruction = detect_audio()\n",
    "    if (instruction != 1 and instruction != -1):\n",
    "        action = closeMatches(commands, instruction)\n",
    "        if action != 1:\n",
    "            return action\n",
    "        else:\n",
    "            return \"Try Again. Could not find command\"\n",
    "    else:\n",
    "        return \"Try Again. Could not recognise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 44  | total loss: \u001b[1m\u001b[32m0.53765\u001b[0m\u001b[0m | time: 0.298s\n",
      "| Adam | epoch: 015 | loss: 0.53765 - acc: 0.8315 -- iter: 128/172\n",
      "Training Step: 45  | total loss: \u001b[1m\u001b[32m0.45923\u001b[0m\u001b[0m | time: 0.465s\n",
      "| Adam | epoch: 015 | loss: 0.45923 - acc: 0.8601 -- iter: 172/172\n",
      "--\n",
      "INFO:tensorflow:C:\\Users\\moham\\Documents\\My Document\\Summer Works\\ITSP\\model.tfl is not in all_model_checkpoint_paths. Manually adding it.\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "anas\n",
      "Say Something\n"
     ]
    }
   ],
   "source": [
    "import tkinter\n",
    "import cv2\n",
    "import PIL.Image, PIL.ImageTk\n",
    "import time\n",
    "\n",
    "class App:\n",
    "    def __init__(self, window, window_title, video_source=0):\n",
    "        self.window = window\n",
    "        self.window.title(window_title)\n",
    "#        self.video_source = video_source\n",
    " \n",
    "         # open video source (by default this will try to open the computer webcam)\n",
    "#        self.vid = MyVideoCapture(self.video_source)\n",
    " \n",
    "         # Create a canvas that can fit the above video source size\n",
    "#        self.canvas = tkinter.Canvas(window, width = self.vid.width, height = self.vid.height)\n",
    "#        self.canvas.pack()\n",
    " \n",
    "         # Button that lets the user take a snapshot\n",
    "        self.btn_snapshot=tkinter.Button(window, text=\"Command\", width=50, command=self.get_command)\n",
    "        self.btn_snapshot.pack(anchor=tkinter.CENTER, expand=True)\n",
    "        \n",
    "        self.output_text = \"\"\n",
    "        self.command = \"\"\n",
    "        self.output_text2 = \"\"\n",
    "        \n",
    "        self.output_field1=tkinter.Label(window, text = self.output_text)\n",
    "        self.output_field1.pack(anchor=tkinter.CENTER, expand=True)\n",
    "        \n",
    "        self.input_field=tkinter.Entry(window)\n",
    "        self.input_field.pack(anchor=tkinter.CENTER, expand=True)\n",
    "        \n",
    "        self.output_field2=tkinter.Label(window, text = self.output_text2)\n",
    "        self.output_field2.pack(anchor=tkinter.CENTER, expand=True)\n",
    "        \n",
    "         # After it is called once, the update method will be automatically called every delay milliseconds\n",
    "#        self.delay = 15\n",
    "#        self.update()\n",
    " \n",
    "        self.window.mainloop()\n",
    "    \n",
    "    def set_output(self):\n",
    "        self.output_field1.configure(text = self.output_text)\n",
    "        \n",
    "    def set_output2(self,text = \"\"):\n",
    "        self.output_field2.configure(text = text)\n",
    " \n",
    "    def get_command(self):\n",
    "        self.command = run()\n",
    "        if self.command == \"add\" or self.command == \"find\":\n",
    "            self.output_text = \"Command:\" + self.command\n",
    "            self.set_output()\n",
    "            if self.command == \"add\":\n",
    "                get_data(self.input_field.get())\n",
    "            if self.command == \"find\":\n",
    "                recognise_face()\n",
    "        else:\n",
    "            self.output_text = self.command\n",
    "            self.set_output()\n",
    "            \n",
    "#\n",
    "#    def update(self):\n",
    "#         # Get a frame from the video source\n",
    "#        ret, frame = self.vid.get_frame()\n",
    "# \n",
    "#        if ret:\n",
    "#            self.photo = PIL.ImageTk.PhotoImage(image = PIL.Image.fromarray(frame))\n",
    "#            self.canvas.create_image(0, 0, image = self.photo, anchor = tkinter.NW)\n",
    "# \n",
    "#        self.window.after(self.delay, self.update)\n",
    "#\n",
    " \n",
    "class MyVideoCapture:\n",
    "    def __init__(self, video_source=0):\n",
    "         # Open the video source\n",
    "        self.vid = cv2.VideoCapture(video_source)\n",
    "        if not self.vid.isOpened():\n",
    "            raise ValueError(\"Unable to open video source\", video_source)\n",
    " \n",
    "         # Get video source width and height\n",
    "        self.width = self.vid.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "        self.height = self.vid.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    " \n",
    "    def get_frame(self):\n",
    "        if self.vid.isOpened():\n",
    "            ret, frame = self.vid.read()\n",
    "            if ret:\n",
    "                # Return a boolean success flag and the current frame converted to BGR\n",
    "                return (ret, cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            else:\n",
    "                return (ret, None)\n",
    "        else:\n",
    "            return (ret, None)\n",
    " \n",
    "     # Release the video source when the object is destroyed\n",
    "    def __del__(self):\n",
    "        if self.vid.isOpened():\n",
    "            self.vid.release()\n",
    " \n",
    " # Create a window and pass it to the Application object\n",
    "App(tkinter.Tk(), \"Tkinter and OpenCV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
